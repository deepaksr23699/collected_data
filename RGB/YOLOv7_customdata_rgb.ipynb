{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "armGo5_S88aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!# Download YOLOv7 code\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd yolov7\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-OlRxrz_LM1",
        "outputId": "83cf6ebb-e94b-41dd-d49b-cabcdbdee339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1197, done.\u001b[K\n",
            "remote: Total 1197 (delta 0), reused 0 (delta 0), pack-reused 1197 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1197/1197), 74.23 MiB | 19.17 MiB/s, done.\n",
            "Resolving deltas: 100% (519/519), done.\n",
            "/content/yolov7\n",
            "cfg\tdetect.py  hubconf.py  models\t  requirements.txt  tools\t  utils\n",
            "data\texport.py  inference   paper\t  scripts\t    train_aux.py\n",
            "deploy\tfigure\t   LICENSE.md  README.md  test.py\t    train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!# Download trained weights\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw_i8OY6_PFG",
        "outputId": "bb083446-935a-4da4-8240-0f6956df39ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-31 04:36:06--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241231T043606Z&X-Amz-Expires=300&X-Amz-Signature=4e3ac490f40f981e69ed1081f7623d1e90084e21c95ce899e63e938332be543d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-31 04:36:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241231T043606Z&X-Amz-Expires=300&X-Amz-Signature=4e3ac490f40f981e69ed1081f7623d1e90084e21c95ce899e63e938332be543d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75587165 (72M) [application/octet-stream]\n",
            "Saving to: ‚Äòyolov7.pt‚Äô\n",
            "\n",
            "yolov7.pt           100%[===================>]  72.08M  27.2MB/s    in 2.6s    \n",
            "\n",
            "2024-12-31 04:36:10 (27.2 MB/s) - ‚Äòyolov7.pt‚Äô saved [75587165/75587165]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organize Dataset"
      ],
      "metadata": {
        "id": "-XOjXdWR8_ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "data = {\n",
        "    'train': '/content/annotations-1/train/images',\n",
        "    'val': '/content/annotations-1/valid/images',\n",
        "    'test': '/content/annotations-1/test/images',\n",
        "    'nc': 2,\n",
        "    'names': ['Person', 'Vehicle4W']\n",
        "}\n",
        "\n",
        "with open('/content/annotations-1/data.yaml', 'w') as file:\n",
        "    documents = yaml.dump(data, file)\n"
      ],
      "metadata": {
        "id": "qT94NTxqB666"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning"
      ],
      "metadata": {
        "id": "AHRkOK-p_nOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbkC0TOH_UQl",
        "outputId": "737dfa18-97b1-4a9a-ec8b-de6e54937625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "img_size = 640  # Image size\n",
        "batch_size = 16  # Batch size\n",
        "epochs = 25  # Number of epochs\n",
        "data_yaml_path = '/content/annotations-1/data.yaml'  # Path to your dataset YAML file\n",
        "weights = 'yolov7.pt'  # Pre-trained weights\n",
        "device = '0'  # Use '0' for first GPU\n",
        "\n",
        "# Run the training script\n",
        "!python train.py --img {img_size} --batch {batch_size} --epochs {epochs} --data {data_yaml_path}  --weights {weights} --device {device}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8wXYu8P_dmm",
        "outputId": "f224a8d0-e4ac-4c49-90be-c162523b8ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-31 04:48:57.054278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-31 04:48:57.075252: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-31 04:48:57.081224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-31 04:48:57.096124: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-31 04:48:58.227915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "YOLOR üöÄ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n",
            "\n",
            "Namespace(weights='yolov7.pt', cfg='', data='/content/annotations-1/data.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=25, batch_size=16, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp4', total_batch_size=16)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
            "/content/yolov7/train.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  run_id = torch.load(weights, map_location=device).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepak-ytsub\u001b[0m (\u001b[33mdeepak-ytsub-indian-institute-of-science\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/yolov7/wandb/run-20241231_044904-emsyhdax\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexp4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepak-ytsub-indian-institute-of-science/YOLOR\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepak-ytsub-indian-institute-of-science/YOLOR/runs/emsyhdax\u001b[0m\n",
            "/content/yolov7/train.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 12                -1  1         0  models.common.MP                        []                            \n",
            " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
            " 25                -1  1         0  models.common.MP                        []                            \n",
            " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 38                -1  1         0  models.common.MP                        []                            \n",
            " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
            " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
            " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 76                -1  1         0  models.common.MP                        []                            \n",
            " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
            " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 89                -1  1         0  models.common.MP                        []                            \n",
            " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
            " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
            " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
            "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
            "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
            "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
            "105   [102, 103, 104]  1     37695  models.yolo.Detect                      [2, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
            "Model Summary: 407 layers, 37200095 parameters, 37200095 gradients\n",
            "\n",
            "Transferred 554/560 items from yolov7.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 95 .bias, 95 conv.weight, 92 other\n",
            "/content/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cache, exists = torch.load(cache_path), True  # load\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/annotations-1/train/labels.cache' images and labels... 2010 found, 0 missing, 187 empty, 0 corrupted: 100% 2010/2010 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/annotations-1/valid/labels.cache' images and labels... 346 found, 0 missing, 34 empty, 0 corrupted: 100% 346/346 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.96, Best Possible Recall (BPR) = 1.0000\n",
            "/content/yolov7/train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=cuda)\n",
            "Image sizes 640 train, 640 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp4\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "  0% 0/126 [00:00<?, ?it/s]/content/yolov7/train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(enabled=cuda):\n",
            "      0/24     1.76G   0.06861   0.00945  0.008151   0.08621        21       640: 100% 126/126 [02:17<00:00,  1.10s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0% 0/11 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.26s/it]\n",
            "                 all         346         312       0.643       0.678       0.622       0.216\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      1/24     12.9G   0.06228  0.005022  0.006065   0.07337        16       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:07<00:00,  1.41it/s]\n",
            "                 all         346         312       0.502      0.0633    0.000614    9.07e-05\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      2/24     15.4G   0.06452  0.004908  0.006832   0.07626        21       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.10it/s]\n",
            "                 all         346         312       0.508     0.00974    0.000862    0.000224\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      3/24     15.4G   0.06454  0.004637  0.006938   0.07612        14       640: 100% 126/126 [01:51<00:00,  1.13it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:07<00:00,  1.53it/s]\n",
            "                 all         346         312     0.00156        0.24    0.000885    0.000166\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      4/24     15.4G   0.06015  0.005372  0.006382   0.07191        17       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.28it/s]\n",
            "                 all         346         312       0.103       0.138       0.039      0.0146\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      5/24     15.4G   0.05584  0.005587  0.004562   0.06599        23       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:08<00:00,  1.33it/s]\n",
            "                 all         346         312       0.774       0.566       0.633       0.198\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      6/24     15.4G   0.05178  0.005411  0.003445   0.06064        28       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.22it/s]\n",
            "                 all         346         312       0.383       0.493       0.392       0.128\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      7/24     15.4G   0.04934  0.005197  0.003756   0.05829        23       640: 100% 126/126 [01:51<00:00,  1.13it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.66it/s]\n",
            "                 all         346         312       0.736       0.706       0.737       0.316\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      8/24     15.4G   0.04638  0.005054  0.003477   0.05491        21       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.23it/s]\n",
            "                 all         346         312       0.812       0.772       0.794       0.398\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      9/24     15.4G   0.05022  0.004839  0.003782   0.05884        20       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:07<00:00,  1.41it/s]\n",
            "                 all         346         312       0.773       0.555       0.634       0.323\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     10/24     15.4G   0.05191  0.004789  0.003902    0.0606        27       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.38it/s]\n",
            "                 all         346         312       0.401        0.37       0.332       0.165\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     11/24     15.4G   0.05095  0.004932  0.003037   0.05892        13       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.61it/s]\n",
            "                 all         346         312       0.871       0.314       0.389       0.159\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     12/24     15.4G   0.04822  0.004983  0.002715   0.05592        12       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  1.99it/s]\n",
            "                 all         346         312       0.642       0.495       0.565       0.286\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     13/24     15.4G   0.05059  0.005194  0.002871   0.05865        17       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.58it/s]\n",
            "                 all         346         312       0.798       0.344        0.41       0.211\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     14/24     15.4G   0.04777  0.005018   0.00273   0.05551        21       640: 100% 126/126 [01:52<00:00,  1.12it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.37it/s]\n",
            "                 all         346         312       0.794        0.67       0.758       0.445\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     15/24     15.4G   0.04829  0.004831  0.002483    0.0556        12       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:07<00:00,  1.44it/s]\n",
            "                 all         346         312        0.87       0.669       0.789       0.485\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     16/24     15.4G   0.04557  0.004848  0.002653   0.05307        24       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.37it/s]\n",
            "                 all         346         312       0.793       0.832        0.91       0.544\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     17/24     15.4G   0.04285   0.00477  0.002398   0.05002        24       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:07<00:00,  1.54it/s]\n",
            "                 all         346         312       0.917       0.712       0.903       0.543\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     18/24     15.4G   0.04233  0.004751  0.002305   0.04938        21       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.07it/s]\n",
            "                 all         346         312       0.898       0.815       0.936       0.568\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     19/24     15.4G   0.04024  0.004999  0.001685   0.04693        24       640: 100% 126/126 [01:48<00:00,  1.16it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.62it/s]\n",
            "                 all         346         312       0.965       0.847       0.945       0.562\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     20/24     15.4G   0.03871  0.004835   0.00182   0.04537        24       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.05it/s]\n",
            "                 all         346         312       0.868       0.863       0.935       0.595\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     21/24     15.4G   0.03777  0.004938  0.001944   0.04465        26       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.59it/s]\n",
            "                 all         346         312       0.948       0.762       0.939       0.618\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     22/24     15.4G   0.03704  0.004712  0.001624   0.04337        21       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.02it/s]\n",
            "                 all         346         312       0.932       0.923       0.963       0.636\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     23/24     15.4G   0.03661  0.004782  0.001533   0.04292        25       640: 100% 126/126 [01:50<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.70it/s]\n",
            "                 all         346         312       0.916        0.86       0.957       0.641\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     24/24     15.4G   0.03564  0.004714   0.00173   0.04209        34       640: 100% 126/126 [01:49<00:00,  1.15it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.78it/s]\n",
            "                 all         346         312       0.936       0.819       0.958       0.657\n",
            "              Person         346         154       0.873       0.981       0.985       0.692\n",
            "           Vehicle4W         346         158           1       0.657       0.931       0.621\n",
            "25 epochs completed in 0.844 hours.\n",
            "\n",
            "/content/yolov7/utils/general.py:802: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  x = torch.load(f, map_location=torch.device('cpu'))\n",
            "Optimizer stripped from runs/train/exp4/weights/last.pt, 74.8MB\n",
            "Optimizer stripped from runs/train/exp4/weights/best.pt, 74.8MB\n",
            "Images sizes do not match. This will causes images to be display incorrectly in the UI.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run_emsyhdax_model (1.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚¢ø\u001b[0m best.pt 944.0KB/71.3MB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading media/images/Results_25_a5b33a612400daacb1b6.png 93.0KB/93.0KB (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading media/images/Results_25_c1746ff04bcd79c53f98.png 152.4KB/152.4KB (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading media/images/Results_25_ecb74ad1efa0389b7c1a.png 94.2KB/94.2KB (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: + 4 more task(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run_emsyhdax_model (2.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£ª\u001b[0m best.pt 1.1MB/71.3MB (1.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run_emsyhdax_model (3.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£Ω\u001b[0m best.pt 2.7MB/71.3MB (2.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run_emsyhdax_model (4.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£æ\u001b[0m best.pt 17.0MB/71.3MB (3.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading history steps 24-25, summary, console lines 256-265 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run_emsyhdax_model (5.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£∑\u001b[0m best.pt 38.7MB/71.3MB (4.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run_emsyhdax_model (6.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚Ü≥ \u001b[38;5;178m‚£Ø\u001b[0m best.pt 60.5MB/71.3MB (5.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run_emsyhdax_model (7.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run_emsyhdax_model (7.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run_emsyhdax_model (7.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall ‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.9582\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.65677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.93646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.81892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.03564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0.00173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.00471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.05337\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0.00732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.00874\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.00114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mexp4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/deepak-ytsub-indian-institute-of-science/YOLOR/runs/emsyhdax\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/deepak-ytsub-indian-institute-of-science/YOLOR\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 406 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241231_044904-emsyhdax/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "EM2H7mFlPch9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --weights /content/yolov7/runs/train/exp4/weights/best.pt --data /content/annotations-1/data.yaml --img 640 --batch 16 --conf 0.25 --task test --device 0 --iou 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOOjjF7jDVTd",
        "outputId": "f293f8b8-ce8f-42cc-b86a-9b99fe59b092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights=['/content/yolov7/runs/train/exp4/weights/best.pt'], data='/content/annotations-1/data.yaml', batch_size=16, img_size=640, conf_thres=0.25, iou_thres=0.5, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
            "YOLOR üöÄ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n",
            "\n",
            "/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(w, map_location=map_location)  # load\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "Model Summary: 306 layers, 36485311 parameters, 6194944 gradients\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/annotations-1/test/labels' images and labels... 390 found, 0 missing, 47 empty, 0 corrupted: 100% 390/390 [00:00<00:00, 778.02it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/annotations-1/test/labels.cache\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:07<00:00,  3.18it/s]\n",
            "                 all         390         343       0.882       0.873       0.905       0.631\n",
            "              Person         390         187       0.819       0.966       0.968       0.669\n",
            "           Vehicle4W         390         156       0.946       0.779       0.842       0.593\n",
            "Speed: 9.8/3.6/13.4 ms inference/NMS/total per 640x640 image at batch-size 16\n",
            "Results saved to runs/test/exp\n"
          ]
        }
      ]
    }
  ]
}